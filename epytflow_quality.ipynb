{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use-Case: Machine Learning for Water Quality State Estimation\n",
    "\n",
    "Water quality state estimation is a crucial task for the successful operation of Water Distribution Networks. However, given the complex dynamics, estimating water quality states in the network is also very challenging.\n",
    "In this context, Machine Learning based approaches offer promising solutions.\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook demonstrates the complete workflow of building a **simple** Machine Learning based water quality state estimator using [EPyT-Flow](https://github.com/WaterFutures/EPyT-Flow) and [TensorFlow](https://www.tensorflow.org/):\n",
    "\n",
    "1. Dataset Generation\n",
    "    - Scenario generation and simulation\n",
    "    - Post-processing to prepare data for Machine Learning\n",
    "2. Training a Machine Learning Model\n",
    "    - Specifying a *Deep Recurrent Neural Network* for water quality state estimation\n",
    "    - Data pre-processing to avoid numerical problems\n",
    "    - Train the *Deep Recurrent Neural Network*\n",
    "3. Evaluation\n",
    "    - Computing several standard evaluation metrics\n",
    "    - Visualize results in a time series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install epyt-flow tensorflow[cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ImportWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, max_error\n",
    "\n",
    "from epyt_flow.simulation import ScenarioSimulator, ToolkitConstants, ScenarioConfig\n",
    "from epyt_flow.topology import NetworkTopology\n",
    "from epyt_flow.data.benchmarks import load_leakdb_scenarios\n",
    "from epyt_flow.utils import to_seconds, plot_timeseries_prediction, plot_timeseries_data, create_path_if_not_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Generation\n",
    "\n",
    "#### Scenario Generation\n",
    "\n",
    "For this notebook, we collect leak-free scenarios from the [LeakDB](https://github.com/KIOS-Research/LeakDB) Hanoi dataset and modify them as follows:\n",
    "\n",
    "- 30 days simulation duration\n",
    "- 30 min intervals\n",
    "- Initial Cl concentration is equal to zero\n",
    "- Set all reaction coefficients to zero\n",
    "- Cl injection (spike pattern) at the reservoir\n",
    "- Flow sensors at all nodes\n",
    "- Cl concentration sensors at all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating spike patterns\n",
    "def create_spike_pattern(pattern_length: int) -> np.ndarray:\n",
    "    steps = np.array([*range(1, pattern_length-1, 1)])\n",
    "    pattern_mult = .5*np.sin(steps*.5) + .5\n",
    "\n",
    "    pattern_mult[8] = 0.0001\n",
    "    spike_pattern = np.copy(pattern_mult[:9])\n",
    "    pattern_mult[9:] = 0\n",
    "\n",
    "    delay = 50  # Spike pattern -- random distance (>= 50) between two spikes\n",
    "    rand_offset = 50\n",
    "    cur_idx = 9\n",
    "\n",
    "    pattern_len = len(pattern_mult)\n",
    "    len_pattern = len(spike_pattern)\n",
    "    while (cur_idx + delay + rand_offset + len_pattern) < pattern_len:\n",
    "        cur_idx += delay + random.randint(0, rand_offset)\n",
    "        pattern_mult[cur_idx:cur_idx+len_pattern] = np.copy(spike_pattern)\n",
    "        cur_idx += len_pattern\n",
    "\n",
    "    return pattern_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a given scenario\n",
    "def run_sim(scenario_config: ScenarioConfig, f_out: str) -> None:\n",
    "    with ScenarioSimulator(scenario_config=scenario_config) as sim:\n",
    "        # Set general parameters\n",
    "        sim.set_general_parameters(simulation_duration=to_seconds(days=30),\n",
    "                                   hydraulic_time_step=to_seconds(minutes=30),\n",
    "                                   reporting_time_step=to_seconds(minutes=30),\n",
    "                                   quality_time_step=to_seconds(minutes=5))\n",
    "\n",
    "        # Make things easy:\n",
    "        # Set initial concentrations to zero\n",
    "        # Set all reaction coefficients to zero\n",
    "        zeroNodes = [0] * sim.epanet_api.getNodeCount()\n",
    "        sim.epanet_api.setNodeInitialQuality(zeroNodes)\n",
    "        sim.epanet_api.setLinkBulkReactionCoeff([0] * sim.epanet_api.getLinkCount())\n",
    "        sim.epanet_api.setLinkWallReactionCoeff([0] * sim.epanet_api.getLinkCount())\n",
    "\n",
    "        # Add chlorine (spike pattern) injection at the reservoir\n",
    "        sim.enable_chemical_analysis()\n",
    "\n",
    "        pattern_length = max(sim.epanet_api.getPatternLengths())\n",
    "        pattern = create_spike_pattern(pattern_length)\n",
    "\n",
    "        reservoir_id = sim.epanet_api.getNodeReservoirNameID()[0]\n",
    "        reservoir_idx = sim.epanet_api.getNodeIndex(reservoir_id)\n",
    "        sim.add_quality_source(node_id=reservoir_id,\n",
    "                               pattern=pattern,\n",
    "                               source_type=ToolkitConstants.EN_CONCEN)\n",
    "\n",
    "        # Place quality and flow sensor at all nodes/links\n",
    "        sim.set_flow_sensors(sensor_locations=sim.sensor_config.links)\n",
    "        sim.set_node_quality_sensors(sensor_locations=sim.sensor_config.nodes)\n",
    "\n",
    "        # Run simulation and store results\n",
    "        res = sim.run_simulation()\n",
    "\n",
    "        sim.get_topology().save_to_file(os.path.join(os.path.dirname(f_out), \"hanoi\"))  # Store topology information\n",
    "\n",
    "        np.savez(f_out,\n",
    "                 injection_node_idx=reservoir_idx - 1,  # NOTE: Indices start at zero in Python, while EPANET starts at 1!\n",
    "                 injection_pattern=pattern,\n",
    "                 node_ids=sim.sensor_config.nodes,\n",
    "                 link_ids=sim.sensor_config.links,\n",
    "                 flow_data=res.get_data_flows(),\n",
    "                 node_quality=res.get_data_nodes_quality())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for storing all generated data\n",
    "path_to_data = \"quality-example-data\"\n",
    "create_path_if_not_exist(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect leak free scenarios from first 100 LeakDB Hanoi scenarios\n",
    "scenarios = load_leakdb_scenarios(range(100), use_net1=False)\n",
    "scenarios = list(filter(lambda s: len(s.system_events) == 0, scenarios))  # Filter out scenarios with leakages\n",
    "print(f\"Number of scenarios: {len(scenarios)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate all scenarios and store results\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    run_sim(scenario, os.path.join(path_to_data, f\"{i}.npz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Before we can build a Machine Learning model, we have to prepare the data accordingly -- i.e. we need input data and the corresponding output.\n",
    "\n",
    "Here, use the current flows at all links and the current Cl concentration at the reservoir as an input to predict Cl concentration (output) at a specified target node. This means that the model has to learn how to use the flow to compute how the Cl travels from the reservoir through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input-output data from simulation results\n",
    "def prepare_data(flows: np.ndarray, cl_conc: np.ndarray, target_node_idx: int,\n",
    "                 injection_nodes_idx: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = []  # Input\n",
    "    y = []  # Output\n",
    "\n",
    "    cur_time = 0\n",
    "    while cur_time < flows.shape[0]:\n",
    "        X.append(np.concatenate((flows[cur_time, :].flatten(),\n",
    "                                 cl_conc[cur_time, injection_nodes_idx].flatten())))\n",
    "        y.append(cl_conc[cur_time, target_node_idx])\n",
    "\n",
    "        cur_time += 1\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we want to predict Cl concentration at node/junction \"25\":\n",
    "\n",
    "![alt text](quality_task.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topology information\n",
    "topo = NetworkTopology.load_from_file(os.path.join(path_to_data, \"hanoi.epytflow_topology\"))\n",
    "\n",
    "# Specify index of the node for which we want to predict the Cl concentration over time\n",
    "target_node_idx = topo.get_all_nodes().index(\"25\")    # We want to predict Cl concentration at node/junction \"25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process simulated scenarios\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(scenarios)):\n",
    "    data = np.load(os.path.join(path_to_data, f\"{i}.npz\"))\n",
    "\n",
    "    X_i, y_i = prepare_data(data[\"flow_data\"], data[\"node_quality\"],\n",
    "                            target_node_idx=target_node_idx,\n",
    "                            injection_nodes_idx=data[\"injection_node_idx\"])\n",
    "\n",
    "    X.append(X_i)\n",
    "    y.append(y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one output (first 500 time steps) for illustrative purposes\n",
    "plot_timeseries_data(np.array(y[0][:500]).reshape(1, -1),\n",
    "                     x_axis_label=\"Time steps (30min)\",\n",
    "                     y_axis_label=\"Cl concentration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to split the data into train, validation, and test sets to properly evaluate the generalizability of our Machine Learning model.\n",
    "\n",
    "Here, we use the first 50% as the training data, the next 25% as the validation set, and the last 25% as the test set that we are going to use to evaluate our Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test set\n",
    "split_points = [int(len(X) / 2), int(len(X) / 2) + int(len(X) / 4)]\n",
    "X_train, X_val, X_test = np.array(X[:split_points[0]]), np.array(X[split_points[0]:split_points[1]]), np.array(X[split_points[1]:])\n",
    "y_train, y_val, y_test = np.array(y[:split_points[0]]), np.array(y[split_points[0]:split_points[1]]), np.array(y[split_points[1]:])\n",
    "\n",
    "print((X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training a Machine Learning Model\n",
    "\n",
    "After having prepared the data, we can finally build and training a Machine Learning model to predict the Cl concentration at a specific node.\n",
    "This involves the following steps:\n",
    "\n",
    "1. Pre-processing the data, i.e. scaling, to avoid numerical problems.\n",
    "2. Specify the Machine Learning model -- here, specifying the *Deep Recurrent Neural Network*.\n",
    "3. Train the Deep Recurrent Neural Network on the training data to predict the Cl concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to avoid numeric instabilities\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
    "\n",
    "X_train = scaler.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a *Deep Recurrent Neural Network* (DRNN) architecture that predicts (over time) the chlorine concentration at the target node based on (over time) all flows and the chlorine injection at the reservoir.\n",
    "\n",
    "The architecture is *recurrent* because it has a memory component -- this memory component allows the neural network to internally model the transport delay.\n",
    "\n",
    "<img src=\"quality_drnn.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the Neural Network architecture\n",
    "class RecurrentDeepNeuralNetwork():\n",
    "    def __init__(self, output_size: int = 1, input_shape: list[int] = None,\n",
    "                 hidden_layer_sizes: list[int] = (128, 64), activation: str = 'tanh'):\n",
    "        self.model = tf.keras.Sequential(\n",
    "                [tf.keras.layers.Input(input_shape)] +        # Input layer\n",
    "                [tf.keras.layers.LSTM(ls, activation=activation, return_sequences=True) # Hidden layer\n",
    "                 for ls in hidden_layer_sizes] +\n",
    "                [tf.keras.layers.Dense(output_size, activation=\"relu\")])    # No negative outputs in the last layer -- Cl concentration cannot be negative!\n",
    "        self.solver = \"adam\"\n",
    "\n",
    "    def save(self, f_out: str = \"recurrent_pred_state.keras\") -> None:\n",
    "        self.model.save(f_out)\n",
    "\n",
    "    def load(self, f_in: str = \"recurrent_pred_state.keras\") -> None:\n",
    "        self.model = tf.keras.models.load_model(f_in)\n",
    "\n",
    "    def fit(self, X_train_flows: np.ndarray, y_train_changes: np.ndarray, n_epochs: int = 500,\n",
    "            callbacks=[], val=None) -> None:   \n",
    "        # Fit model by minimizing loss function -- i.e. mean squared error\n",
    "        self.model.compile(optimizer=self.solver, loss=tf.keras.losses.MeanSquaredError(), metrics=[\"mse\"])\n",
    "        self.model.fit(X_train_flows, y_train_changes, epochs=n_epochs, verbose=True, callbacks=callbacks, validation_data=val)\n",
    "\n",
    "    def __call__(self, X_flows: np.ndarray) -> np.ndarray:\n",
    "        return self.predict(X_flows)\n",
    "\n",
    "    def predict(self, X_flows: np.ndarray) -> np.ndarray:\n",
    "        return self.model(X_flows, training=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a neural network for predicting the Cl concentration\n",
    "model = RecurrentDeepNeuralNetwork(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# Fit neural network for at most 100 iterations (early stopping will stop training if there is no improvement for 10 iterations)\n",
    "earlystopping_mgr = tf.keras.callbacks.EarlyStopping(monitor='val_mse', min_delta=0, patience=10, verbose=0, mode='min',\n",
    "                                                     baseline=None, restore_best_weights=True, start_from_epoch=0)\n",
    "model.fit(X_train, y_train, n_epochs=100, callbacks=[earlystopping_mgr], val=(X_val, y_val))\n",
    "\n",
    "# Save final (trained) model\n",
    "model.save(os.path.join(path_to_data, f\"model_node{target_node_idx}.keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation\n",
    "\n",
    "As the last step, we evaluate our trained Neural Network on the unseen test data set to assess its generalizability -- i.e. can be predict the Cl concentration for unseen scenarios (i.e. different demand pattern and different spike pattern)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standard evaluation metrics\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test.flatten(), y_test_pred.flatten())}\\n\" +\n",
    "      f\"R^2: {r2_score(y_test.flatten(), y_test_pred.flatten())}\\n\" +\n",
    "      f\"MAB: {mean_absolute_error(y_test.flatten(), y_test_pred.flatten())}\\n\" +\n",
    "      f\"ME: {max_error(y_test.flatten(), y_test_pred.flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_id = 0   # Index of test scenario -- note that we have 6 test scenarios\n",
    "\n",
    "# Time window to plot -- i.e. first 1000 time steps\n",
    "t0 = 0\n",
    "t1 = 1000\n",
    "\n",
    "# Plot ground truth and predicted Cl concentration\n",
    "#plot_timeseries_data(y_test[s_id, t0:t1].reshape(1, -1))\n",
    "plot_timeseries_prediction(y_test[s_id, t0:t1].flatten(), y_test_pred[s_id, t0:t1].flatten(),\n",
    "                           x_axis_label=\"Time steps (30min)\", y_axis_label=\"Cl concentration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watertut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
